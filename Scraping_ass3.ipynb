{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>Web Scraping - Assignment 3</h1>\n",
    "</div>\n",
    "\n",
    "### **Author:** Péter Bence Török\n",
    "\n",
    "###  **CEU ID:** 2404748\n",
    "\n",
    "The aim of this project is to scrape the [ENF Solar website](https://www.enfsolar.com/pv/panel), a comprehensive online repository of solar panel products, to gather detailed technical data about various panels. The retrieved data will be analyzed to evaluate and compare the technical specifications of solar panels, providing insights into their performance and key features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up environment and creating user-defined functions\n",
    "\n",
    "\n",
    "In this section, I will define the user-defined functions and set up the necessary environment required for the web scraping process. These functions will streamline repetitive tasks, such as data extraction, while ensuring the environment is properly configured with the necessary libraries and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required python modules\n",
    "from scrapethat import *\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve links from a specific page number\n",
    "def get_urls(n):\n",
    "    # Initialize an empty list to store URLs\n",
    "    urls = []\n",
    "    \n",
    "    # Read the content of the homepage for the given page number 'n'\n",
    "    homepage_url = read_cloud(f\"https://www.enfsolar.com/pv/panel/{n}\")\n",
    "    \n",
    "    # Extract the links to individual products using CSS selectors\n",
    "    # Select all elements with the class 'enf-product-name' and construct full URLs\n",
    "    links = ['https://www.enfsolar.com' + x['href'] for x in homepage_url.select('.enf-product-name')]\n",
    "    \n",
    "    # Add the extracted links to the URLs list\n",
    "    urls.extend(links)\n",
    "    \n",
    "    # Return the list of URLs\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that returns technical data for each product and handles missing variables\n",
    "def get_product_details(url):\n",
    "    df = []\n",
    "    html = read_cloud(url)\n",
    "    \n",
    "    # Select the table with data\n",
    "    table = html.select('.enf-api-spec-cell')\n",
    "    unique_data_ids = list(set(td.get('data-id') for td in table if td.get('data-id')))\n",
    "    \n",
    "    for data_id in unique_data_ids:\n",
    "        try:\n",
    "            # Safely extract values for the current product\n",
    "            one_product = [\n",
    "                str(x).split('>')[1].split('<')[0].strip() \n",
    "                for x in table if data_id in str(x)\n",
    "            ]\n",
    "            \n",
    "            data = {}\n",
    "            \n",
    "            # Product Name\n",
    "            data['Product_name'] = one_product[0] if len(one_product) > 0 else None\n",
    "            \n",
    "            # Product Family\n",
    "            try:\n",
    "                title_section = html.select('.mk-title')[0]\n",
    "                h1 = title_section.find('h1')\n",
    "                data['Product_family'] = h1['title'] if h1 and 'title' in h1.attrs else None\n",
    "            except (IndexError, AttributeError):\n",
    "                data['Product_family'] = None\n",
    "\n",
    "            # Manufacturer\n",
    "            try:\n",
    "                data['Manufacturer'] = title_section.find('span', {'id': 'mkjs-product-profile'}).get_text(strip=True)\n",
    "            except (AttributeError, IndexError):\n",
    "                data['Manufacturer'] = None\n",
    "\n",
    "            # Price\n",
    "            try:\n",
    "                price_section = html.select('.yellow')\n",
    "                data['Price'] = next(\n",
    "                    (span.find('b').text.strip() for span in price_section if span.find('b')), \n",
    "                    None\n",
    "                )\n",
    "            except (StopIteration, AttributeError):\n",
    "                data['Price'] = None\n",
    "\n",
    "            # Technology\n",
    "            try:\n",
    "                tech_table = html.select('.enf-pd-profile-mini-table')[0]\n",
    "                rows = tech_table.find_all('tr')\n",
    "                data['Technology'] = rows[0].find('td').text.strip() if len(rows) > 0 else None\n",
    "                data['Region'] = rows[2].find('td').text.strip() if len(rows) > 2 else None\n",
    "            except (IndexError, AttributeError):\n",
    "                data['Technology'] = None\n",
    "                data['Region'] = None\n",
    "\n",
    "            # Maximum Power, Voltage, Panel Efficiency\n",
    "            data['Maximum_Power(Pmax)'] = one_product[1] if len(one_product) > 1 else None\n",
    "            data['Voltage_at_Maximum_Power(Vmpp)'] = one_product[2] if len(one_product) > 2 else None\n",
    "            data['Panel Efficiency'] = one_product[6] if len(one_product) > 6 else None\n",
    "            \n",
    "            # Similar Products Count and Link\n",
    "            data['Similar_products'] = len(unique_data_ids)\n",
    "            data['Link'] = url\n",
    "            \n",
    "            # Append the collected product data\n",
    "            df.append(data)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data_id {data_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scraping of the websites\n",
    "In this section, I will carry out a two-phase web scraping process. The first phase involves collecting links from a specified range of pages, while the second phase extracts detailed product information from those links, with progress tracked dynamically throughout. The chosen website employs scraping protection and occasionally blocks the read_cloud function. To handle this, the code automatically retries the queries and reprocesses the blocked pages until all data is successfully retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of pages to scrape, ranging from 1 to 50\n",
    "pages = [r for r in range(1, 51)]\n",
    "\n",
    "# Initialize empty lists for storing links and final data\n",
    "links = []\n",
    "df_f = []\n",
    "\n",
    "# Variable to track progress for the first phase\n",
    "done = 0\n",
    "n = len(pages)  # Total number of pages to scrape\n",
    "\n",
    "# Loop through each page to gather links\n",
    "for x in pages:\n",
    "    # Call the get_urls function to fetch links from the current page\n",
    "    link = get_urls(x)\n",
    "    \n",
    "    # If no links are found, re-add the page to the list for retrying\n",
    "    if link == []:\n",
    "        pages.append(x)\n",
    "    else:\n",
    "        # Extend the list of links with the retrieved links\n",
    "        links.extend(link)\n",
    "        done += 1  # Increment the progress counter\n",
    "        \n",
    "        # Clear the output and display the progress\n",
    "        clear_output(wait=True)\n",
    "        print(f\"First phase: {(done/n)*100}% is ready\")\n",
    "        \n",
    "        # Pause for 3 seconds to avoid overwhelming the server\n",
    "        time.sleep(3)\n",
    "\n",
    "# Update the total number of links collected\n",
    "n = len(links)\n",
    "\n",
    "# Print completion message for the first phase\n",
    "print(f'First phase is done. Result: {n} links')\n",
    "\n",
    "# Reset the progress tracker for the second phase\n",
    "done = 0\n",
    "\n",
    "# Loop through each link to scrape product details\n",
    "for url in links:\n",
    "    # Call the get_product_details function to extract data from the current URL\n",
    "    data = get_product_details(url)\n",
    "    \n",
    "    # If no data is found, re-add the URL to the list for retrying\n",
    "    if data == []:\n",
    "        links.append(url)\n",
    "    else:\n",
    "        # Extend the final data list with the retrieved product details\n",
    "        df_f.extend(data)\n",
    "        done += 1  # Increment the progress counter\n",
    "        \n",
    "        # Clear the output and display the progress\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Second phase: {(done/n)*100}% is ready\")\n",
    "\n",
    "df=pd.DataFrame(df_f)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving dataframe to local .csv file\n",
    "df.to_csv('solar_panels.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
